{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Agents and Enviroments\n",
    "\n",
    "## Agents\n",
    "An _agent_ is anything the perceives its _enviroment_ through _sensors_ and interacting with it through _actuators_.\n",
    "\n",
    "| Agent | Sensors | Actuators |\n",
    "| :--- | :--- | :--- |\n",
    "| Human | eyes, ears, ... | hands, legs, ... |\n",
    "| Robot | camera, ir range finder, ... | motors, pistons, ... |\n",
    "| Software | keystrokes, files, packets, ... | displaying to screen, writing files, sending packets, ... |\n",
    "\n",
    "The term _percept_ refers to an agents inputs at a given instant, a _percept sequence_ is the complete history of inputs. The _agent function_ is an abstract mathmatical definition that maps a given percept sequence to an action, the _agent program_ is the concrete implementation.\n",
    "\n",
    "A _performance measure_ evaluates the desirability of any given sequence of enviroment states, for example a robot vacume would be rewared for cleaning a section and penalized for the power and noise it made. A _rational agent_ is an agent that selects and action that is expected to maximize its performance measure, given the percept sequence and built-in knowledge.\n",
    "\n",
    "The _task envrioment_ or _PEAS_ (performance, enviroment, actuators, sensors) is a descrition of a agents rationality. For example a PEAS description for an automated taxi drived would be:\n",
    "\n",
    "| Agent Type | Performance Measure | Enviroment | Actuators | Sensors |\n",
    "| :--- | :--- | :--- | :--- | :--- |\n",
    "| Taxi driver | Safe, fast, legal, confortable, maximize profit | Roads, traffic, pedestrians, customers | Steering, accelerator, brake, indicators, horn, display | Cameras, sonar, speedometer, GPS, odometer, accelerometer, engine sensors, keyboard |\n",
    "\n",
    "### Simple-reflex agent\n",
    "Actions depend only on immedient percepts, implemented with condition-action rules.\n",
    "\n",
    "```\n",
    "function SIMPLE-REFLEX-AGENT(percept) returns an action\n",
    "    persistent: rules, a set of condition-action rules\n",
    "    \n",
    "    state <- INTERPRET-INPUT(percept)\n",
    "    rule <- RULE-MATCH(state, rules)\n",
    "    action <- rule.ACTION\n",
    "    \n",
    "    return action\n",
    "```\n",
    "\n",
    "### Model-based reflex agent\n",
    "To handle partial observability is for the agent to keep track of the parts it cant see with some internal state.\n",
    "\n",
    "```\n",
    "function MODEL-BASED-REFLEX-AGENT(percept) returns an action\n",
    "    persistent: state, an agents current conception of the world state\n",
    "                model, a description of how the next state depends on current state and action\n",
    "                rules, a set of condition-action rules\n",
    "                action, the most recent action\n",
    "                \n",
    "    state <- UPDATE-STATE(state, action, percept, model)\n",
    "    rule <- RULE-MATCH(state, rules)\n",
    "    action <- rule.ACTION\n",
    "    \n",
    "    return action\n",
    "```\n",
    "\n",
    "### Goal-based agents\n",
    "So far agents have implicit goals, goal-based agents have variable goals that they have to form plans to solve.\n",
    "\n",
    "Agents now need to consider:\n",
    "- How the world involves when certain actions are taken\n",
    "- Is taking an action desirable\n",
    "\n",
    "### Utility-based agents\n",
    "Goal-based agents aren't enough for good behaviour in complex enviroments. Their are many sequences of actions that complete a goal, but some may be more desirable than others. Utility-based agents have a utility function that measures the desirability of a world state.\n",
    "\n",
    "### Learning agents\n",
    "Some problems are to large to program, learning agents have a _learning element_ which makes improvements based on the feedback from the _critic_ and the _performance element_ which is what we previously considered as the whole agent. The _problem generator_ suggests actions that may lead to new experiences which aid in the learning process.\n",
    "\n",
    "## Enviroments\n",
    "\n",
    "### Fully vs partially observable\n",
    "If an agents sensors return the full state of the enviroment, the task enviroment is _fully observable_, otherwise the task enviroment is _paritally ovservable_ or even _unobservable_.\n",
    "\n",
    "### Single vs multi agent\n",
    "Solving a crossword is an example of a _single agent_ enviroment, whereas chess is a _multi agent_ enviroment. The distinction between what is and is not modelled as an agent is determined by its behaviour and whether it can be best described as maximizing a performance measure whos values depends on the other agent.\n",
    "\n",
    "### Compentative vs cooprative\n",
    "Chess is an example of a _competative_ mulitagent enviroment because maximizing one players performance measure, minimizes the others. Self dricing taxis is an example of a _cooprative_ multiagent enviroment, since avoiding collisions maximizes the performance measures of all agents.\n",
    "\n",
    "### Deterministic vs stochastic\n",
    "An enviroment that is fully determined by the current state and an agents action is said to be _deterministic_ otherwise it is _stochastic_. If an enviroment is not fully observable or not deterministic it is said to be _uncertain_. A _stochastic_ enviroment generally implies the uncertainty can be expressed in terms of probability, an enviroment where this is not the case is said to be _nondeterministic_.\n",
    "\n",
    "### Episodic vs sequential\n",
    "Agents the perform a single action depending only on the current percept is said to have an _episodic_ task enviroment. An example would be an agent that spots defective parts on an assembly line. In _sequential_ enviroments, current decisions effect all future decisions, for example a single move in chess effects the moves the player has availble in the future.\n",
    "\n",
    "### Static vs dynamic\n",
    "If the enviroment does not change while an agent is deciding, it is said to be _static_, otherwise its _dynamic_. In a dynamic enviroment continuly ask the agent what to do, and the time spent deciding is equivalent to doing nothing. Static enviroments are simpler since they dont need to continly percept the enviroment or model the passage of time. If the enviroment doesnt change with the passage of time but an agents performance measure does, the task enviroment is said to be _semidynamic_. Chess is static, however when played with a clock it is semidynamic.\n",
    "\n",
    "### Discrete vs continuous\n",
    "Chess has a _discrete_ percepts (state of the board) and a _discrete_ set of actions, where as self driving cars have _continuous_ percepts (countinuously varying intensities in camera inputs etc) and _continuous_ actions (steering actions etc).\n",
    "\n",
    "### Known vs unknown\n",
    "For a _known_ eviroment, the outcomes for all actions are given. In an _unknown_ enviroment the agent must learn how it works to make good decisions. Note that an _known_ enviroment doesnt imply its fully observable, an agent can know all the rules of solitare, however the draw pile cannot be observed.\n",
    "\n",
    "### Examples\n",
    "\n",
    "| Task Enviroment | Fully/partially observable | Single/multi agents | Deterministic/stochastic | Episodic/sequential | Static/dynamic | Discrete/continuous |\n",
    "| :--- | :--- | :--- | :--- | :--- |\n",
    "| Crossword puzzle | Fully | Single | Deterministic | Sequential | Static | Discrete |\n",
    "| Chess (with clock) | Fully | Multi | Deterministic | Sequential | Semi | Discrete |\n",
    "| Poker | Partially | Multi | Deterministic | Sequential | Static | Discrete |\n",
    "| Taxi driving | Partially | Multi | Stochastic | Sequential | Dynamic | Continuous |\n",
    "| Medical diagnosis | Partially | Single | Stochastic | Sequential | Dynamic | Continuous |\n",
    "| Part-picking robot | Partially | Single | Stochastic | Episodic | Dynamic | Continuous |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
